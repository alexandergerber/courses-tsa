---
title: 'ARMA Models'
description: ""
---

## Fitting ARMA Models

```yaml
type: NormalExercise
key: 0fd6ee0f58
xp: 100
```

After we removed trend and seasonality the time series of construction supplies appears to be stationary.
Hence, we can start to use ARMA models to analyse the short term dependencies. For this we will continue to work with 
the residuals of the linear trend + seasonality model `con_supply2010_random1`.  

The function `arima()` is the standard tool to fit ARMA(p,q) models in R. 
The syntax is as follows

```
arima(time_series, order = c(p,0,q))
```

`@instructions`
- Fit an ARMA(1,1) model to `con_supply2010_random1`and save the fitted model as `arma11`.
- Compute summary statistics for the model by using the function `summary()`.
- Extract the fitted values of the estimated model by using the function `fitted()` and assign the result to `arma11_fit`.
- Extract the residuals of the estimated model by using the function `residuals()` and assign the result to `arma11_res`.

`@hint`


`@pre_exercise_code`
```{r}
library(quantmod)
library(forecast)
con_supply <- getSymbols("IPB54100N", src = "FRED", auto.assign = FALSE)
con_supply_ts  <- ts(con_supply, start = c(1947, 1), frequency = 12)
con_supply2010 <- window(con_supply_ts, start = c(2010, 1))
seasonal_model <- tslm(con_supply2010 ~ trend + season)
con_supply2010_random1 <- residuals(seasonal_model)

```

`@sample_code`
```{r}
# Fit an arma11 model to con_supply2010_random1


# Compute summary statistics


# Extract the residuals


# Extract the residuals

```

`@solution`
```{r}
# Fit a arma 11 model to con_supply2010_random1
arma11 <- arima(con_supply2010_random1)

# Compute summary statistics
summary(arma11)

# Extract the residuals
arma11_res <- residuals(arma11)

## Extract the residuals
arma11_fit <- fitted(arma11)

```

`@sct`
```{r}
success_msg("Great!")
ex() %>% check_object("arma11") %>% check_equal()
ex() %>% check_object("arma11_res") %>% check_equal()
ex() %>% check_object("arma11_fit") %>% check_equal()
ex() %>% check_function("summary") %>% check_arg("object") %>% check_equal()
```

---

## Simulate an ARMA Time Series

```yaml
type: NormalExercise
key: 8fec5a03b9
xp: 100
```

Before we continue to work on the construction supply data we will use R to learn more about the properties of ARMA time series. 
This will help us to decide which ARMA model we should use (i.e how to choose $p$ and $q$). 

We start by simulating data from an ARMA process. R provides the function `arima.sim(model, n)` which can be used to do that.
The parameter `model` expects a list of the form 
``
list(ar = c(phi1, phi2, ...), ma = c(theta1, theta2, ...))
``
and `n` is the length of the time series that will be generated. The function returns an object of class `ts`.

Example: 
If we want to simulate $1000$ observations from the ARMA(1,2) process
$$y _t = 0.4 y _{t-1} +  \epsilon _t + 0.6\epsilon _{t-1} + 0.3\epsilon _{t-2} $$
we can do this with
```
arima.sim(model = list(ar =c(0.6), ma = c(0.6, 0.3)), n = 1000)
```

`@instructions`
- Simulate data from the process 
$$y _t = 0.9 y _{t-1} +  \epsilon _t,\quad t = 1,...,1000$$ and save the result as `ar1` 
- Simulate data from the process 
 $$y _t = 0.5 y _{t-1} + 0.4 y _{t-2} +  \epsilon _t,\quad t = 1,...,1000$$ and save the result as `ar2`
- Simulate data from the process 
$$y _t = \epsilon _t + 0.6\epsilon _{t-1} , \quad t = 1,...,1000$$ and save the result as `ma1` 
- Simulate data from the process 
 $$y _t = \epsilon _t + 0.6\epsilon _{t-1} + 0.3\epsilon _{t-2}, \quad t = 1,...,1000$$ and save the result as `ma2`

`@hint`


`@pre_exercise_code`
```{r}

```

`@sample_code`
```{r}
set.seed(1)
# Simulate the AR(1) time series


# Simulate the AR(2) time series


# Simulate the MA(1) time series


# Simulate the MA(2) time series
```

`@solution`
```{r}
set.seed(1)
# Simulate the AR(1) time series
ar1 <- arima.sim(1000, model = list(ar = c(0.9)))

# Simulate the AR(2) time series
ar2 <- arima.sim(1000, model = list(ar = c(0.5, 0.4)))

# Simulate the MA(1) time series
ma1 <- arima.sim(1000, model = list(ma = c(0.6)))

# Simulate the MA(2) time series
ma2 <- arima.sim(1000, model = list(ma = c(0.6, 0.3)))
```

`@sct`
```{r}
ex() %>% check_object("ar1") %>% check_equal()
ex() %>% check_object("ar2") %>% check_equal()
ex() %>% check_object("ma1") %>% check_equal()
ex() %>% check_object("ma2") %>% check_equal()
success_msg("Great!")
```

---

## Plot the Simulated ARMA Data

```yaml
type: NormalExercise
key: 79e8ae9ec7
xp: 100
```

You have simulated AR and MA time series. Now we want to find out how they can be distinguished. 
A first step is plotting the data. 

It is easier to compare the different plots if we put them next to each other. To achieve this for plots
which are generated by the autoplot function we can use the function `grid.arrange()`. The syntax is as follows: 
```
grid.arrange(
	autoplot(y1), 
    autoplot(y2), 
    ...
    autoplot(yn)
 )
```

Note: `grid.arrange()` is a function contained in the package `gridExtra`, which is already loaded.

`@instructions`
- Plot `ar1`, `ar2`, `ma1` and `ma2` and next to each other by using the function `grid.arrange()` and look how they differ

`@hint`


`@pre_exercise_code`
```{r}
library(gridExtra)
library(forecast)
set.seed(1)
# Simulate the AR(1) time series
ar1 <- arima.sim(model = list(ar = c(0.9)), n = 1000)

# Simulate the AR(2) time series
ar2 <- arima.sim(model = list(ar = c(0.5, 0.4)), n = 1000)

# Simulate the MA(1) time series
ma1 <- arima.sim(model = list(ma = c(0.6)), n = 1000)

# Simulate the MA(2) time series 
ma2 <- arima.sim(model = list(ma = c(0.6, 0.3)), n = 1000)
```

`@sample_code`
```{r}
# Plot all 4 series next to each other






```

`@solution`
```{r}
# Plot all 4 series next to each other
grid.arrange(
  autoplot(ar1),
  autoplot(ar2), 
  autoplot(ma1),
  autoplot(ma2)
)
```

`@sct`
```{r}
ex() %>% check_function("autoplot", index  = 1) %>% check_arg("object") %>% check_equal()
ex() %>% check_function("autoplot", index  = 2) %>% check_arg("object") %>% check_equal()
ex() %>% check_function("autoplot", index  = 3) %>% check_arg("object") %>% check_equal()
ex() %>% check_function("autoplot", index  = 4) %>% check_arg("object") %>% check_equal()
success_msg("You're the king of arima.sim!")
```

---

## ACF and PACF of AR and MA Models

```yaml
type: NormalExercise
key: fc82ca2ade
xp: 100
```

Based on the plots we can see that time series generated from an AR-process are generally smoother and are more persistent compared to MA-processes. 
However, it is hard to distinguish an AR(1) form an AR(2) or a MA(1) from a MA(2) process. Furthermore it could also be hard to distinguish an AR process with 
e.g. $\phi = 0.2$ from an MA process by mere inspection of the plot.

However, knowledge about the characteristic ACF and PACF of different kind of models can help to determine the model order.
The function `ARMAacf()` can be used to compute the theoretical ACF or PACF of any ARMA model. 
The syntax for the ACF of an ARMA model (which of course includes AR(p) and MA(q) models as special cases) is
```
ARMAacf(ar = c(phi1, phi2, ...), ma = c(theta1, theta2, ...), lag = number_of_lags)
```
To obtain the PACF add `pacf = TRUE` to the function call.

`@instructions`
Compute the first 10 values of the ACF and PACF of the following processes 
\begin{align}
y _t &= 0.5 y _{t-1} + \epsilon _t \\\\
y _t &= 0.4 y _{t-1} + 0.3 y _{t-2} + \epsilon _t \\\\
y _t &=  \epsilon _t + 0.8 \epsilon _{t-1} \\\\
y _t &=  \epsilon _t + 1.5 \epsilon _{t-1} - 0.6 \epsilon _{t-2} 
\end{align}
What are the differences?

`@hint`


`@pre_exercise_code`
```{r}

```

`@sample_code`
```{r}
# ACF and PACF of the AR(1) process



# ACF and PACF of the AR(2) process



# ACF and PACF of the MA(1) process



# ACF and PACF of the MA(2) process


```

`@solution`
```{r}
# ACF and PACF of an AR(1)
ARMAacf(ar = c(0.5), lag = 10)
ARMAacf(ar = c(0.5), lag = 10, pacf = TRUE)

# ACF and PACF of an AR(2)
ARMAacf(ar = c(0.4, 0.3), lag = 10)
ARMAacf(ar = c(0.4, 0.3), lag = 10, pacf = TRUE)

# ACF and PACF of an MA(1)
ARMAacf(ma = c(0.8), lag = 10)
ARMAacf(ma = c(0.8), lag = 10, pacf = TRUE)

# ACF and PACF of an MA(2)
ARMAacf(ma = c(1.5, -0.6), lag = 10)
ARMAacf(ma = c(1.5, -0.6), lag = 10, pacf = TRUE)
```

`@sct`
```{r}
ex() %>% check_function("ARMAacf", index  = 1) %>% {
  check_arg(., "ar") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
}

ex() %>% check_function("ARMAacf", index  = 2) %>% {
  check_arg(., "ar") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
  check_arg(., "pacf") %>% check_equal()
}

ex() %>% check_function("ARMAacf", index  = 3) %>% {
  check_arg(., "ar") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
  }

ex() %>% check_function("ARMAacf", index  = 4) %>% {
  check_arg(., "ar") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
  check_arg(., "pacf") %>% check_equal()
}

ex() %>% check_function("ARMAacf", index  = 5) %>% {
  check_arg(., "ma") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
}

ex() %>% check_function("ARMAacf", index  = 6) %>% {
  check_arg(., "ma") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
  check_arg(., "pacf") %>% check_equal()
}

ex() %>% check_function("ARMAacf", index  = 7) %>% {
  check_arg(., "ma") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
}


ex() %>% check_function("ARMAacf", index  = 8) %>% {
  check_arg(., "ma") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
  check_arg(., "pacf") %>% check_equal()
}
success_msg("Well done!")
```

---

## ACF and PACF of an ARMA Process

```yaml
type: NormalExercise
key: c3e61c892f
xp: 100
```

In the previous exercise we found out that  
- the ACF of an AR(p)-process decays slowly 
- the PACF of an AR(p)-process has a cutoff after the p-th lag 
- the ACF of an MA(q)-process  has a cutoff after the q-th lag 
- the PACF of an MA(p)-process decays slowly 

But what about ARMA(p,q) processes?

`@instructions`
Compute the ACF and the PACF of the process
$$y _t = 0.8 y _{t-1} +  \epsilon _t + 1.5 \epsilon _{t-1} - 0.6 \epsilon _{t-2}$$

`@hint`


`@pre_exercise_code`
```{r}

```

`@sample_code`
```{r}
# Compute ACF and PACF of the ARMA process


```

`@solution`
```{r}
# Compute ACF and PACF of the ARMA process
ARMAacf(ar = 0.8, ma = c(1.5, -0.6), lag = 10)
ARMAacf(ar = 0.8,ma = c(0.7, -0.6), lag = 10, pacf = TRUE)
```

`@sct`
```{r}
ex() %>% check_function("ARMAacf", index  = 1) %>% {
  check_arg(., "ar") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
}

ex() %>% check_function("ARMAacf", index  = 2) %>% {
  check_arg(., "ar") %>% check_equal()
  check_arg(., "lag.max") %>% check_equal()
  check_arg(., "pacf") %>% check_equal()
}
success_msg("ARMAzing!")
```

---

## ACF and PACF of a Simulated AR Time Series

```yaml
type: NormalExercise
key: fd98368e58
xp: 100
```

To choose an appropriate model we can make use of the knowledge about the theoretical ACF and PACF. 
In practice we do not know these functions, however, we can use estimates of the ACF and PACF in order to try to spot if a characteristic feature of one of the here considered models is observable.  

The ACF/PACF can be estimated and plotted by the functions `ggAcf() ggPacf()` of the `forecast` package.
In the 2. exercise of this chapter we already simulated some AR and MA time series.

`@instructions`
- Plot the ACF and PACF of `ar1` and `ar2` next to each other by using the functions `ggAcf()`, `ggPacf()` and `grid.arrange()`
- Do the same for `ma1` and `ma2`

`@hint`


`@pre_exercise_code`
```{r}
library(gridExtra)
library(forecast)
set.seed(1)
# Simulate the AR(1) time series
ar1 <- arima.sim(model = list(ar = c(0.9)), n = 1000)
# Simulate the AR(2) time series
ar2 <- arima.sim(model = list(ar = c(0.5, 0.4)), n = 1000)
# Simulate the MA(1) time series
ma1 <- arima.sim(model = list(ma = c(0.6)), n = 1000)
# Simulate the MA(2) time series 
ma2 <- arima.sim(model = list(ma = c(0.6, 0.3)), n = 1000)
```

`@sample_code`
```{r}
# Plot the ACF/PACF of the AR(1) and AR(2) time series





# Plot the ACF/PACF of the MA(1) and MA(2) time series





```

`@solution`
```{r}
# Plot the ACF/PACF of the AR(1) and AR(2) time series
grid.arrange(
  ggAcf(ar1), ggPacf(ar1), 
  ggAcf(ar2), ggPacf(ar2)
)

# Plot the ACF/PACF of the MA(1) and MA(2) time series
grid.arrange(
  ggAcf(ma1), ggPacf(ma1), 
  ggAcf(ma2), ggPacf(ma2)
)

```

`@sct`
```{r}
success_msg("Congrats!")
```

---

## ACF and PACF of a Simulated ARMA Time Series

```yaml
type: NormalExercise
key: 5d66faf365
xp: 100
```

As we found out earlier the ACF and PACF of an ARMA(p,q) model decay both slowly to zero without an obvious cutoff. Hence, we can distinguish an ARMA time series from an AR and MA time series. The same should be visible for their estimated counterparts.

`@instructions`
- Simulate data from from the process 
$$y _t = 0.7 y _{t-1} +  \epsilon _t + 0.5 \epsilon _{t-1},\quad t = 1,...,1000$$ and save the result as `arma11` 
- Plot the ACF and the PACF next to each other using `grid.arrange()`

`@hint`


`@pre_exercise_code`
```{r}
library(quantmod)
library(forecast)
library(gridExtra)
con_supply <- getSymbols("IPB54100N", src = "FRED", auto.assign = FALSE)
con_supply_ts  <- ts(con_supply, start = c(1947, 1), frequency = 12)
con_supply2010 <- window(con_supply_ts, start = c(2010, 1))
seasonal_model <- tslm(con_supply2010 ~ trend + season)
con_supply2010_random1 <- residuals(seasonal_model)
```

`@sample_code`
```{r}
set.seed(3)
# Simulate a ARMA(1,1) time series


# Plot ACF and PACF

```

`@solution`
```{r}
set.seed(3)
# Simulate a ARMA(1,1) time series
arma11 <- arima.sim(model = list(ar = 0.7, ma = 0.5), n = 1000)

# Plot ACF and PACF
grid.arrange(
  ggAcf(arma11),
  ggPacf(arma11)
)
```

`@sct`
```{r}
ex() %>% check_object("arma11") %>% check_equal()
ex() %>% check_function("grid.arrange") %>% {
  check_arg(., "...")
} %>% check_equal()
success_msg("Fancy")
```

---

## Information Criteria

```yaml
type: NormalExercise
key: fb20c21242
xp: 100
```

We can detect ARMA(p,q)-models based on ACF/PACF since neither ACF nor PACF has a clear cutoff. However, it does not tell us which model orders to choose. 
Also for pure AR and MA time series the answers provided may not be as clear-cut as one would like (e.g. if the sample size is low). 
For this reason we might look for another way to select the best suited model. A very common choice for model selection are information criteria such as 
the Akaike Information Criterion (AIC). The AIC deals with the trade-off between model fit and complexity. 
The idea is to fit a variety of plausible candidate models and then choose the model which provides the "best" fit without overfitting the data.
If we base our decision on information criteria such as AIC we would choose among all candidate models the one with the lowest value for that criterion. 
The AIC can be computed by passing the fitted model to the function `AIC()`.

`@instructions`
- The working environment contains the series `y`. Fit an AR(1), MA(1) and ARMA(1,1) model to the data and save the results as `ar1`, `ma1` and `arma11`. 
- Compute the AIC for each model. Which model would you choose?

`@hint`


`@pre_exercise_code`
```{r}
y <- arima.sim(model = list(ma = 0.9), n = 1000)
```

`@sample_code`
```{r}
# Fit the 3 models




# Compute the AIC for each model



```

`@solution`
```{r}
# Fit the 3 models
ar1 <- arima(y, order = c(1,0,0))
ma1 <- arima(y, order = c(0,0,1))
arma11 <- arima(y, order = c(1,0,1))
# Compute the AIC for each model
AIC(ar1)
AIC(ma1)
AIC(arma11)
```

`@sct`
```{r}
ex() %>% check_object("ar1") %>% check_equal()
ex() %>% check_object("ma1") %>% check_equal()
ex() %>% check_object("arma11") %>% check_equal()
ex() %>% check_function("AIC", index  = 1) %>% check_arg("object") %>%check_equal()
ex() %>% check_function("AIC", index  = 2) %>% check_arg("object")%>% check_equal()
ex() %>% check_function("AIC", index  = 3) %>% check_arg("object")%>% check_equal()
success_msg("Wonderful!")
```

---

## Choose a Model for the Construction Supply Data

```yaml
type: NormalExercise
key: 0f890d38ef
xp: 100
```

One possible way how to proceed in practice is as follows: 

1. Estimate the ACF and PACF and analyse the result.
2. If ACF/PACF already provide enough information, choose the suggested model.  
3. If ACF/PACF don't give a clear answer, choose appropriate candidate models and use the AIC.

`@instructions`
- Estimate ACF/PACF for the `con_supply2010_random1`. This already provides a clear answer for this example. 
- Estimate the model suggested by `ACF/PACF` and save the model object as `final_model`
- Check if the AIC would come to the same result. For this estimate an ARMA(1,0), ARMA(0,1), ARMA(1,1) and compute the AIC for each of those models. Then save the numerical value of the best AIC as `best_AIC`.

`@hint`


`@pre_exercise_code`
```{r}
library(quantmod)
library(forecast)
library(gridExtra)
con_supply <- getSymbols("IPB54100N", src = "FRED", auto.assign = FALSE)
con_supply_ts  <- ts(con_supply, start = c(1947, 1), frequency = 12)
con_supply2010 <- window(con_supply_ts, start = c(2010, 1))
seasonal_model <- tslm(con_supply2010 ~ trend + season)
con_supply2010_random1 <- residuals(seasonal_model)
```

`@sample_code`
```{r}
# ACF and PACF



# Estimate the final model 


# Check if AIC comes to same result




# Best AIC

```

`@solution`
```{r}
# ACF and PACF
ggAcf(con_supply2010_random1)
ggPacf(con_supply2010_random1)

# Estimate the final model 
final_model <- arima(con_supply2010_random1, order = c(1,0,0))

# Check if AIC comes to same result
AIC(arima(con_supply2010_random1, order = c(1,0,0)))
AIC(arima(con_supply2010_random1, order = c(0,0,1)))
AIC(arima(con_supply2010_random1, order = c(1,0,1)))

# Best AIC
best_AIC <- AIC(arima(con_supply2010_random1, order = c(1,0,0)))
```

`@sct`
```{r}
ex() %>% check_function("ggAcf") %>% check_arg("x") %>% check_equal()
ex() %>% check_function("ggPacf") %>% check_arg("x") %>% check_equal()
ex() %>% check_object("final_model") %>% check_equal()
ex() %>% check_object("best_AIC") %>% check_equal()
success_msg(":)")
```

---

## Model Diagnostics with ACF and PACF

```yaml
type: NormalExercise
key: e29c07d14d
xp: 100
```

After we found a model it is good practice to run some diagnostic checks. Even if we found the most appropriate ARMA model it is not safe to assume that ARMA models were a good choice in the first place (i.e. it could be that all ARMA models are inappropriate).

The fitted model is appropriate if the residuals behave in a way that is consistent with the model. In our case this means that the residuals 
should look as if they where generated by a white noise process. 

One way to check this is again to estimate the ACF. For large $n$ the sample autocorrelation of a White Noise series is approximately $N(0,1/n)$ distributed. 
In order to test to the 5% level if the residuals are white noise we could count how many values fall outside the displayed bounds. If e.g more than 5 out of 
100 displayed autocorrelations are outside the bounds we would reject the hypothesis that the residuals are white noise.

`@instructions`
- Extract the residuals of the final model 
- Plot the ACF of the residuals and decide if they look like white noise

`@hint`


`@pre_exercise_code`
```{r}
library(quantmod)
library(forecast)
con_supply <- getSymbols("IPB54100N", src = "FRED", auto.assign = FALSE)
con_supply_ts  <- ts(con_supply, start = c(1947, 1), frequency = 12)
con_supply2010 <- window(con_supply_ts, start = c(2010, 1))
seasonal_model <- tslm(con_supply2010 ~ trend + season)
con_supply2010_random1 <- residuals(seasonal_model)
final_model <- auto.arima(con_supply2010_random1)
```

`@sample_code`
```{r}
# Extract the residuals of the final model


# Plot the ACF of the residuals and decide if they look like white noise


```

`@solution`
```{r}
# Extract the residuals of the final model
final_model_resid <- residuals(final_model)

# Plot the ACF of the residuals and decide if they look like white noise
ggAcf(final_model_resid)

```

`@sct`
```{r}
ex() %>% check_object("final_model_resid") %>% check_equal()
ex() %>% check_function("ggAcf") %>% check_arg("x") %>% check_equal()
success_msg("Great!")
```

---

## Ljung-Box Test

```yaml
type: NormalExercise
key: 006d98c393
xp: 100
```

Another way of testing whether the residuals resemble white noise is to use a hypothesis test such as the Ljung-Box test. 
The null hypothesis that the data are uncorrelated is tested against the alternative that autocorrelation is present.
The idea behind the test is to accumulate the autocorrelations at each individual lag to one test statistic in order to test the overall autocorrelation in the data. 
There is one critical choice to make: How many lags to include into the test statistic. However, as a rule of thumb you can choose the number of lags as $\sqrt{T}$, where $R$ is the length of the time series.  

To perform the test in R the function  `Box.test()` can be used with the following syntax:

```
Box.test(ts, lag = number_of_lags, type = "Ljung-Box")
```

`@instructions`
- Test $H_0$ whether there is autocorrelation left in `final_model_resid` of the last exercise using the Ljung-Box test with the rule of thumb from above

`@hint`


`@pre_exercise_code`
```{r}
library(quantmod)
library(forecast)
con_supply <- getSymbols("IPB54100N", src = "FRED", auto.assign = FALSE)
con_supply_ts  <- ts(con_supply, start = c(1947, 1), frequency = 12)
con_supply2010 <- window(con_supply_ts, start = c(2010, 1))
seasonal_model <- tslm(con_supply2010 ~ trend + season)
con_supply2010_random1 <- residuals(seasonal_model)
final_model <- auto.arima(con_supply2010_random1)
final_model_resid <- residuals(final_model)
```

`@sample_code`
```{r}
# Perform the Ljung-Box test

```

`@solution`
```{r}
# Perform the Ljung-Box test
Box.test(final_model_resid, lag = sqrt(length(final_model_resid)), type = "Ljung-Box")
```

`@sct`
```{r}
ex() %>% check_function("Box.test") %>%{
  check_arg(., "x") %>% check_equal()
  check_arg(., "lag") %>% check_equal()
  check_arg(., "type") %>% check_equal()
  }
success_msg("Great")
```

---

## Shortcut Forecast Package

```yaml
type: NormalExercise
key: a7a4e557dc
xp: 100
```

For both model selection and model diagnostic, the `forecast` package provides convenience functions which do most of what we have done in the previous exercises automatically. 
The first one is `auto.arima()`. This function fits a lot of different models and returns the model with the best result in terms of a specified criteria (e.g. AIC).

The second one is `checkresiduals()`. If provided with a model object this function extracts the residuals, performs a Ljung-Box test and plots the ACF. 

The syntax is as follows:
```
fitted_model <- auto.arima(time_series, ic = "aic") 
checkresiduals(fitted_model)
```

`@instructions`
- Use `auto.arima()` to find the best model for `con_supply2010_random1` based on the AIC and save it as `final_model`
- Perform some diagnostic checks using `checkresiduals()` on the residuals of the estimated model

`@hint`


`@pre_exercise_code`
```{r}
library(quantmod)
library(forecast)
library(gridExtra)
con_supply <- getSymbols("IPB54100N", src = "FRED", auto.assign = FALSE)
con_supply_ts  <- ts(con_supply, start = c(1947, 1), frequency = 12)
con_supply2010 <- window(con_supply_ts, start = c(2010, 1))
seasonal_model <- tslm(con_supply2010 ~ trend + season)
con_supply2010_random1 <- residuals(seasonal_model)

```

`@sample_code`
```{r}

```

`@solution`
```{r}
# Fit the model using auto.arima
final_model <- auto.arima(con_supply2010_random1, ic = "aic")

# Perform diagnostic checks
final_model %>% checkresiduals()

```

`@sct`
```{r}
ex() %>% check_object("final_model") %>% check_equal()
ex() %>% check_function("checkresiduals") %>% check_arg("object") %>% check_equal()
success_msg("Badass!")
```
